---
title: "Practical Machine Learning"
author: "Matthew Hill"
date: "6/7/2020"
output: html_document
---

## Prediction Assignment

### Background
Using devices such as JawboneUp, NikeFuelBand, and Fitbitit is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in
their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  
   
In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).   

### Preparing the data and R packages  

#### Load packages, set caching 

```{r, message=FALSE}
require(caret)
require(stats)
require(knitr)
require(ggplot2)
require(corrplot)
require(Rtsne)
require(xgboost)
knitr::opts_chunk$set(cache=TRUE)
```
"XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance." (*https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/*)   

#### Getting Data
```{r}
# URL of the training and testing data
train.url ="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# file names
train.name = "./data/pml-training.csv"
test.name = "./data/pml-testing.csv"
# if directory does not exist, create new
if (!file.exists("./data")) {
  dir.create("./data")
}
# if files does not exist, download the files
if (!file.exists(train.name)) {
  download.file(train.url, destfile=train.name, method="curl")
}
if (!file.exists(test.name)) {
  download.file(test.url, destfile=test.name, method="curl")
}
# load the CSV files as data.frame 
train = read.csv("./data/pml-training.csv")
test = read.csv("./data/pml-testing.csv")
dim(train)
dim(test)
```  

The raw training data has 19,622 observations and 158 predictors. Column `X` is just the row numbers. We will be applying the model from the training data on the testing data, which has 20 rows and the same 158 features. The target outcome is named `classe`.   

#### Data cleaning

First, extract target outcome (the activity quality) from training data, so now the training data contains only the predictors (the activity monitors).   
```{r}
# target outcome (label)
outcome.org = train[, "classe"]
outcome = outcome.org 
levels(outcome)
```
Outcome has 5 levels in character format.   
Because the XGBoost gradient booster only recognizes numeric data, we will convert the 'classe' levels to numeric.   
```{r}
# convert character levels to numeric
num.class = length(levels(outcome))
levels(outcome) = 1:num.class
head(outcome)
```
   
The outcome column is removed from training data.   
```{r}
# remove outcome from train
train$classe = NULL
```

The assignment asks to use data from accelerometers on the `belt`, `forearm`, `arm`, and `dumbell`, we will match these keywords on the predictors and extract them for our model.   
  
```{r}
# filter columns on: belt, forearm, arm, dumbell
filter = grepl("belt|forearm|arm|dumbell", names(train))
train = train[, filter]
test = test[, filter]
```

Remove all columns with NA values.   
```{r}
# remove columns with NA
cols.without.na = colSums(is.na(test)) == 0
train = train[, cols.without.na]
test = test[, cols.without.na]
```

### Preprocessing  

#### Check for features's variance

Based on the PCA (principal component analysis), it is important that features have maximum variance for maximum uniqueness, so that each feature is as distant as possible (as orthogonal as possible) from the other features.   
```{r}
# check for zero variance
zero.var = nearZeroVar(train, saveMetrics=TRUE)
zero.var
```
All features have enough variability, FALSE for zeroVar. So we will keep these features.  

#### Plot of relationship between features and outcome  

Plot the relationship between features and outcome. From the plot below, each feature has relatively the same distribution among the 5 outcome levels (A, B, C, D, E).   
```{r fig.width=12, fig.height=8, dpi=72}
featurePlot(train, outcome.org, "strip")
```

#### Plot of correlation matrix  

Plot a correlation matrix between features.   
A good set of features is when they are highly uncorrelated (orthogonal). The plot below shows average of correlation is not too high, so there isn't a need to perform further PCA preprocessing.   
```{r fig.width=12, fig.height=12, dpi=72}
corrplot.mixed(cor(train), lower="circle", upper="color", 
               tl.pos="lt", diag="n", order="hclust", hclust.method="complete")
```

#### tSNE plot 

A tSNE (t-Distributed Stochastic Neighbor Embedding) visualization is a 2D plot of multidimensional features. In the tSNE plot below there is no clear separation of clustering of the 5 levels of outcome (A, B, C, D, E). This indicates a need for robust machine learning to make our predictions.

```{r fig.width=12, fig.height=8, dpi=72}
# t-Distributed Stochastic Neighbor Embedding
tsne = Rtsne(as.matrix(train), check_duplicates=FALSE, pca=TRUE, 
              perplexity=30, theta=0.5, dims=2)
embedding = as.data.frame(tsne$Y)
embedding$Class = outcome.org
g = ggplot(embedding, aes(x=V1, y=V2, color=Class)) +
  geom_point(size=1.25) +
  guides(colour=guide_legend(override.aes=list(size=6))) +
  xlab("") + ylab("") +
  ggtitle("t-SNE 2D Embedding of 'Classe' Outcome") +
  theme_light(base_size=20) +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank())
print(g)
```

### Build machine learning model 

Now build a machine learning model to predict activity quality (`classe` outcome) from the activity monitors (the features or predictors) by using XGBoost extreme gradient boosting algorithm.    

#### XGBoost data

XGBoost supports only numeric matrix data.

```{r}
# convert data to matrix
train.matrix = as.matrix(train)
mode(train.matrix) = "numeric"
test.matrix = as.matrix(test)
mode(test.matrix) = "numeric"
# convert outcome from factor to numeric matrix 
# XGBoost takes multi-labels in [0, numOfClass)
y = as.matrix(as.integer(outcome)-1)
```

#### XGBoost parameters 

Set XGBoost parameters for cross validation and training.  
Set a multiclass classification objective as the gradient boosting's learning function.   
Set evaluation metric to `merror`, multiclass error rate.   

```{r}
# XGBoost parameters
param <- list("objective" = "multi:softprob",    # multiclass classification 
              "num_class" = num.class,    # number of classes 
              "eval_metric" = "merror",    # evaluation metric 
              "nthread" = 8,   # number of threads to be used 
              "max_depth" = 16,    # maximum depth of tree 
              "eta" = 0.3,    # step size shrinkage 
              "gamma" = 0,    # minimum loss reduction 
              "subsample" = 1,    # part of data instances to grow tree 
              "colsample_bytree" = 1,  # subsample ratio of columns when constructing each tree 
              "min_child_weight" = 12  # minimum sum of instance weight needed in a child 
              )
```

#### Expected error rate 

Expected error rate is less than `1%` for a good classification. Do cross validation to estimate the error rate using 4-fold cross validation, with 200 epochs to reach the expected error rate of less than `1%`.  

#### 4-fold cross validation  

```{r}
# set random seed, for reproducibility 
set.seed(3333)
# k-fold cross validation, with timing
nround.cv = 200
system.time( bst.cv <- xgb.cv(param=param, data=train.matrix, label=y, 
              nfold=4, nrounds=nround.cv, prediction=TRUE, verbose=FALSE) )
```

Elapsed time is around 65 seconds.  

```{r}
tail(bst.cv$evaluation_log) 
```
   
From the cross validation, choose index with minimum multiclass error rate.  
Index will be used in the model training to fulfill expected minimum error rate of `< 1%`.  
```{r}
# index of minimum merror
min.merror.idx = which.min(bst.cv$evaluation_log[, test_merror_mean]) 
min.merror.idx 
# minimum merror
bst.cv$evaluation_log[min.merror.idx,]
```
Best cross-validation's minimum error rate `test_merror_mean` is around 0.006 (0.6%), happened at 181st iteration.   

#### Confusion matrix 

Tabulates the cross-validation's predictions of the model against the truths.  

```{r}
# get CV's prediction decoding
pred.cv = matrix(bst.cv$pred, nrow=length(bst.cv$pred)/num.class, ncol=num.class)
pred.cv = max.col(pred.cv, "last")
# confusion matrix
confusionMatrix(factor(y+1), factor(pred.cv))
```

Confusion matrix shows concentration of correct predictions is on the diagonal, as expected.  
  
The average accuracy is `99.46%`, with error rate is `0.54%`. So, expected error rate of less than `1%` is fulfilled.  

#### Model training 

Fit the XGBoost gradient boosting model on all of the training data.   
```{r}
# real model fit training, with full data
system.time( bst <- xgboost(param=param, data=train.matrix, label=y, 
                           nrounds=min.merror.idx, verbose=0) )
```
Time elapsed is around 17 seconds.  

#### Predicting the testing data

```{r}
# xgboost predict test data using the trained model
pred <- predict(bst, test.matrix)  
head(pred, 10)  
```

#### Post-processing

Output of prediction is the predicted probability of the 5 levels (columns) of outcome.  
Decode the quantitative 5 levels of outcomes to qualitative letters (A, B, C, D, E).   
  
```{r}
# decode prediction
pred = matrix(pred, nrow=num.class, ncol=length(pred)/num.class)
pred = t(pred)
pred = max.col(pred, "last")
pred.char = toupper(letters[pred])
```

(*The prediction result `pred.char` is not displayed intentionally due to Honour Code, because it is the answer of the "project submission" part.*)   

#### Feature importance

```{r fig.width=8, fig.height=12, dpi=72}
# get the trained model
model = xgb.dump(bst, with_stats=TRUE)
# get the feature real names
names = dimnames(train.matrix)[[2]]
# compute feature importance matrix
importance_matrix = xgb.importance(names, model=bst)
# plot
gp = xgb.plot.importance(importance_matrix)
print(gp) 
```

Feature importance plot is useful to select only best features with highest correlation to the outcome(s). To improve model fitting performance (time or overfitting), less important features can be removed.   

------------------   